# A simple feed-forward neural network in MAMBA, the MPC language of
# the SCALE-MAMBA library.
#
# The algorithm is an adapted verison of the one presented here:
# https://github.com/ludobouan/pure-numpy-feedfowardNN

# Author: Florian Apfelbeck, August, 2018

# TODO:
# - !!code need serious cleaning!!! Variable names, main function, proper dot
# function, maybe some assignments like array <> array with for_range as
# function to have this cleaner
# - adjust / align variables with slr code (e.g., targets vs. predictions)
# - for loops might be faster when using directly variables and go not via
# layer_dimensions[][]. Like this, for loops are maybe compiled with a fix
# end/sized circuit -> check it out

# doing class for NN not recommended sinc 'method_block does not allow output
# of matrix' and without method block, compiling takes a long time

from Compiler import mpc_math
import numpy as np

# Set MAX_ARRAY_LENGTH at least to the maximum length of your data input. Then
# adapt the bitsize of this number.  The bitsize is required for the random
# function, which only takes Python native int, which therefore has to be preset.
MAX_ARRAY_LENGTH = 10000
MAX_ARRAY_LENGTH_NBIT = 14

# TODO make mem addresses consistent
MEM_ADDRESS = 0

# NULL choosen here as -123456789. Be careful and change if your dataset
# contains this number.  This is necessary since sint.Array() treats python
# None like 0 (i.e., sint(None) is same as sint(0))
NULL = -123456789

EULER = 2.71828

# Length of mantissa in bits
sfloat.vlen = 15
# Length of exponent in bits
sfloat.plen = 10
# Statistical security parameter for floats
sfloat.kappa = 4

# Numpy random seed
np.random.seed(1)

### PART 1: BASICS ###
# Definitions of class to support functions later.
###
# class NeuralNetwork():
#     def __init__(self):
#         self.weights = sfloat.Array(MAX_ARRAY_LENGTH)
#         self.num_layers = 1
#         self.adjustments = sfloat.Array(MAX_ARRAY_LENGTH)

def add_layer(in_dim, neuron_dim):
    # Create weights + biases
    shape = (in_dim, neuron_dim)
    n = MemValue(num_hidden_layer)

    # Create weights and biases (not secret here yet, numpy used )
    weights_numpy = 2 * np.random.random(shape) - 1
    biases_numpy = 2 * np.random.random((1, shape[1])) - 1
    print(weights_numpy)

    # Convert weight matrix from numpy to sfix
    # i = 0
    i = MemValue(0)
    j = MemValue(0)
    i.write(0)
    for row in weights_numpy:
        j.write(0)
        for column in row:
            val_sfloat = sfloat(column)
            # print_ln("n: %s", n)
            # print_ln("i: %s", i)
            # print_ln("j: %s", j)
            weights[n][i][j] = sfix(val_sfloat)
            j.write(j + 1)
        i.write(i + 1)

    # Convert biases from numpy to sfix
    i.write(0)
    for column in biases_numpy[0]:
        val_sfloat = sfloat(column)
        biases[n][i] = sfix(val_sfloat)
        i.write(i + 1)

    num_hidden_layer.write(num_hidden_layer + 1)
    # print_ln("++++++++++++++++")
    # @for_range(9)
    #     print_ln("w_1[%s] %s ", i, weights[0][0][i].reveal())
    # def _(i):
    #
    # @for_range(9)
    # def _(i):
    #     print_ln("w_2[%s]: %s", i, weights[1][0][i].reveal())
    #
    # @for_range(9)
    # def _(i):
    #     print_ln("b[%s]: %s", i, biases[0][i].reveal())
    # print_ln("++++++++++++++++")

def __sigmoid(x):
    return sfix(1) / (sfix(1) + mpc_math.exp2_fx(-x * mpc_math.log2_fx(sfix(EULER))))

def __sigmoid_derivative(x):
    return sfix(sfloat(x) * (sfloat(1) - sfloat(x)))

def dot_prod(x_array, y_matrix, layer):
    output = sfix.Array(max_dim_largest_layer)
    # TODO: maybe better do range of layer dimensions and not the full thing?
    # Rows of output has dimensions of columns of weights (y_matrix)
    sum_ = sfix.Array(1)

    # For debugging:
    # @for_range(9)
    # def _(i):
    #     print_ln("x_arr[%s]: %s", i, x_array[i].reveal())
    #     print_ln("y_matrix[%s]: %s", i, y_matrix[i][0].reveal())

    # Even if assigned within this function, output keeps its values from
    # the last call of this function. It's cleaner to set is everytime to 0.0.
    # Like that, data_temp in --__forward_propagate function only has the values
    # which actually matter (i.e., for output layer, only data_temp[0] has an
    # entry, since output layer has size 1).
    # Other option would be to only assign the correct layer dimensions,
    # therefore keep the data of the previous layer in data_temp and just call
    # the right dimension then from data_temp. SEE BELOW[*]
    # @for_range(max_dim_largest_layer)
    # def _(m):
    #     output[m] = 0.0

    # Compute dot product
    @for_range(layer_dimensions[layer][1])
    def _(i):
        sum_[0] = sfloat(0.0)

        @for_range(layer_dimensions[layer][0])
        def _(j):
            product = sfloat(x_array[j]) * sfloat(y_matrix[j][i])
            sum_[0] = sum_[0] + product
        output[i] = sfix(sum_[0])

    # SEE ABOVE[*]. Still here for testing. TODO: to be removed at one point
    @for_range(layer_dimensions[layer][1])
    @for_range(max_dim_largest_layer)
    def _(i):
        x_array[i] = output[i]

def dot_product_MxA(matrix, array, output, layer):
    @for_range(layer_dimensions[layer][0])
    def _(i):
        @for_range(layer_dimensions[layer][1])
        def _(j):
            product = sfloat(matrix[i][j]) * sfloat(array[j])
            output[i] = output[i] + sfix(product)

def dot_prodAxA_T(x_matrix, y_matrix, output_matrix, layer):
    @for_range(layer_dimensions[layer][1])
    def _(i):
        @for_range(layer_dimensions[layer][0])
        def _(j):
            output_matrix[i][j] = sfix(sfloat(x_matrix[layer][i]) * sfloat(y_matrix[layer][j]))


def __forward_propagate(data, input_iteration, activation_values):
    # activation_values multiarray includes the first (input) layer (at 0), contrary to
    # weights and biases (1st hidden layer at n=0)
    # activation_values 2D-matrix (n,i) with n number of layer, and i neuron
    # within that layer n
    # activation_values = sfix.Matrix(tot_num_layers, max_dim_largest_layer)
    data_temp = sfix.Array(max_dim_largest_layer)

    # For debugging:
    # @for_range(9)
    # def _(i):
    #     print_ln("data[%s]: %s", i, data[input_iteration][i].reveal())

    # print_ln("input iteration: %s", input_iteration)

    # print_ln("len(data[0]): %s ", len(data[0]))
    @for_range(len(data[0]))
    def _(i):
        data_temp[i] = data[input_iteration][i]

    # Assign data to activation_values
    @for_range(len(data[0]))
    def _(i):
        activation_values[0][i] = data[input_iteration][i]
    # For Debugging: checks that assignment is correct
    # @for_range(3)
    # def _(i):
    #     print_ln("a[%s] %s %s %s", i, activation_values[i][0].reveal(), activation_values[i][1].reveal(), activation_values[i][1].reveal())

    @for_range(tot_num_hidden_layers)
    def _(layer):
        # Layer refers to layer in activation_values here
        layer = layer + 1
        # Sum weights and biases
        # sum_weight_bias = sfix.Matrix(max_dim_largest_layer, max_dim_largest_layer)

        # For debugging:
        # print_ln("layer dim %s", layer_dimensions[layer-1][1])
        # print_ln("Layer number %s", layer)

        # print_ln("data_temp before dot")
        # @for_range(12)
        # def _(i):
        #     print_ln("d_t bef dot %s", data_temp[i].reveal())

        # Dot product
        dot_prod(data_temp, weights[layer-1], layer-1)

        # print_ln("data_temp after dot")
        # @for_range(12)
        # def _(i):
        #     print_ln("d_t after dot%s", data_temp[i].reveal())
        #

        @for_range(layer_dimensions[layer-1][1])
        def _(i):
            data_temp[i] = data_temp[i] + biases[layer-1][i]

        @for_range(layer_dimensions[layer-1][1])
        def _(i):
            data_temp[i] = __sigmoid(data_temp[i])

        # For Debugging:
        # After first hidden layer  (so before output) of first iteration of
        # data, so training_data[0][x] and layer = 2, data_temp[0] has
        # to be same as print statement in python code with
        # print("data out after sigmoid")
        # @for_range(9)
        # def _(i):
        #     print_ln("d_t sig %s", data_temp[i].reveal())
#########################
        @for_range(layer_dimensions[layer-1][1])
        def _(i):
            activation_values[layer][i] = data_temp[i]

# Sum squared error
def sum_squared_error(outputs, target_vector):
    # For debugging:
    # @for_range(len(outputs))
    # def _(i):
    #     print_ln("outputs[%s]: %s", i, outputs[i].reveal())
    #
    # @for_range(len(target_vector))
    # def _(i):
    #     print_ln("target_vector[%s]: %s", i, target_vector[i].reveal())
    #
    # print_ln("len target_vector %s", len(target_vector))

    # target_vector_err = sfloat.Array(len(target_vector))
    sum_err = sfloat.Array(1)
    sum_err[0] = 0.0
    @for_range(len(target_vector))
    def _(i):
        # print_ln("print outp again %s", outputs[i].reveal())
        # print_ln("print targ again %s", target_vector[i].reveal())
        targets_err = sfloat(outputs[i]) - sfloat(target_vector[i])
        # print_ln("targ_err: %s", targets_err.reveal())
        # might need sfloats here
        sum_err[0] = sum_err[0] + (targets_err ** 2)

    # print_ln("sum_err: %s", sum_err[0].reveal())
    mean_err = sum_err[0] / sfloat(len(target_vector))
    # print_ln("mean_err: %s", mean_err.reveal())
    # TODO: check if this can be float..
    return sfix(sfloat(0.5) * mean_err)

# def print_array(array):
#     @for_range(layer_dimensions[tot_num_hidden_layers-1][1])
#     def _(i):
#         print_ln("target vector[%s]: %s", i, target_vector[i].reveal())

def multiply(x_array, y_array, output, layer):
    @for_range(layer_dimensions[layer][0])
    def _(i):
        output[i] = sfix(sfloat(x_array[i]) * sfloat(y_array[i]))





def __back_propagate(activ_val_matrix, target_vector):
    # deltas[number of layer, excl. input][number of neuron/delta].
    # Layers are [0]: 1st hidden layer, [last]: output layer
    deltas = sfix.Matrix(tot_num_hidden_layers, max_dim_largest_layer)
    weights_layer = sfix.Matrix(max_dim_largest_layer, max_dim_largest_layer)
    prev_deltas = sfix.Array(max_dim_largest_layer)
    a_val = sfix.Array(max_dim_largest_layer)
    sigmoid_deriv = sfix.Array(max_dim_largest_layer)
    dot_product = sfix.Array(max_dim_largest_layer)
    multiply_out = sfix.Array(max_dim_largest_layer)


    # For debugging:
    # print_ln("++++++++activation value matrix++++++++++")
    # @for_range(tot_num_layers)
    # def _(j):
    #     print_ln("act_val_mat[%s]: %s, %s, %s, %s, %s, %s", j, activ_val_matrix[j][0].reveal(), activ_val_matrix[j][1].reveal(), activ_val_matrix[j][2].reveal(), activ_val_matrix[j][3].reveal(), activ_val_matrix[j][4].reveal(), activ_val_matrix[j][8].reveal())

    # print_ln("layer dim prev last layer %s", layer_dimensions[tot_num_hidden_layers-1][1])
    # print_ln("layer dim prev last layer[0] %s", layer_dimensions[tot_num_hidden_layers-1][0])

    # @for_range(layer_dimensions[tot_num_hidden_layers-1][1])
    # def _(i):
    #     print_ln("target vector[%s]: %s", i, target_vector[i].reveal())
    #
    # @for_range(layer_dimensions[tot_num_hidden_layers-1][1])
    # def _(i):
    #     print_ln("activ_mat[%s]: %s", i, activ_val_matrix[tot_num_layers-1][i].reveal())

    # Assign output / activation_values - targets of last layer to last layer of deltas
    @for_range(layer_dimensions[tot_num_hidden_layers-1][1])
    def _(i):
        deltas[tot_num_hidden_layers-1][i] = activ_val_matrix[tot_num_layers-1][i] - target_vector[i]

    # For debugging:
    # @for_range(9)
    # def _(i):
    #     print_ln("deltas 1[%s]: %s", i, deltas[tot_num_hidden_layers-1][i].reveal())

    layer = MemValue(tot_num_hidden_layers - 1)

    # For debugging:
    # print_ln("layer no: %s", layer)
    # shouldn't it also go to input layer, i.e., taking the first weights into
    # account??
    # For now: all layers except input and output.
    @for_range(tot_num_hidden_layers - 1)
    def _(i):
        # reverse, therefore not i is used but layer
        # Set weights_layer to 0. Safety measure.
        @for_range(len(weights_layer))
        def _(j):
            @for_range(len(weights_layer[0]))
            def _(k):
                weights_layer[j][k] = 0.0

        # Set dot_product and multiply_out o 0. Safety measure.
        @for_range(len(dot_product))
        def _(j):
            dot_product[j] = 0.0
            multiply_out[j] = 0.0


        # Assign acivation / output values of layer to a_val
        @for_range(layer_dimensions[layer][0])
        def _(j):
            a_val[j] = activ_val_matrix[layer][j]

        # Assign weights of layer to weights_layer
        @for_range(layer_dimensions[layer][0])
        def _(j):
            @for_range(layer_dimensions[layer][1])
            def _(k):
                weights_layer[j][k] = weights[layer][j][k]

        # Assign previous deltas. Layer here same as for weights, since deltas
        # only has hidden and output layer, no input layer
        @for_range(layer_dimensions[layer][1])
        def _(j):
            prev_deltas[j] = deltas[layer][j]

        # For debugging:
        # @for_range(9)
        # def _(j):
        #     @for_range(9)
        #     def _(k):
        #         print_ln("weights_layer[%s][%s]: %s", j, k, weights_layer[j][k].reveal())

        # For debugging:
        # @for_range(9)
        # def _(j):
        #     print_ln("a_val[%s]: %s", j, a_val[j].reveal())

        # For debugging:
        # @for_range(9)
        # def _(j):
        #     print_ln("prev_deltas[%s] %s", j, prev_deltas[j].reveal())

        @for_range(layer_dimensions[layer][0])
        def _(j):
            sigmoid_deriv[j] = __sigmoid_derivative(a_val[j])

        # For debugging:
        # @for_range(9)
        # def _(j):
        #     print_ln("sig_deriv[%s]: %s", j, sigmoid_deriv[j].reveal())

        # dot product (matrix x array)
        dot_product_MxA(weights_layer, prev_deltas, dot_product, layer)

        # For debugging:
        # @for_range(9)
        # def _(j):
        #     print_ln("dot_prod[%s]: %s", j, dot_product[j].reveal())

        multiply(dot_product, sigmoid_deriv, multiply_out, layer)

        # For debugging:
        # @for_range(9)
        # def _(j):
        #     print_ln("multipl[%s]: %s", j, multiply_out[j].reveal())

        @for_range(layer_dimensions[layer][0])
        def _(j):
            deltas[layer-1][j] = multiply_out[j]

        # For debugging:
        # @for_range(9)
        # def _(j):
        #     print_ln("deltas[%s][%s]: %s", layer-1, j, deltas[layer-1][j].reveal())
        layer.write(layer - 1)


#######
    dot_product_matrix = sfix.Matrix(max_dim_largest_layer, max_dim_largest_layer)

    @for_range(tot_num_hidden_layers)
    def _(layer):
        @for_range(len(dot_product_matrix))
        def _(i):
            @for_range(len(dot_product_matrix[0]))
            def _(j):
                dot_product_matrix[i][j]

        # Dot product arrayxarray.t (transposed)
        dot_prodAxA_T(deltas, activ_val_matrix, dot_product_matrix, layer)

        # For debugging:
        # @for_range(9)
        # def _(i):
        #     @for_range(9)
        #     def _(j):
        #         print_ln("dot_prod_m[%s][%s]: %s", i, j, dot_product_matrix[i][j].reveal())

        # Adjustments does not take last layer, therefore, has same layout as
        # weights 3dArray. Dot_product matrix has to be transposed.
        @for_range(layer_dimensions[layer][0])
        def _(i):
            @for_range(layer_dimensions[layer][1])
            def _(j):
                adjustments[layer][i][j] = adjustments[layer][i][j] + dot_product_matrix[j][i]

    # print_ln("+++++++++++adjustments++++++++++++++")
    # @for_range(2)
    # def _(i):
    #     @for_range(9)
    #     def _(j):
    #         @for_range(9)
    #         def _(k):
    #             print_ln("adjustments[%s][%s][%s]: %s", i, j, k, adjustments[i][j][k].reveal())


def __gradient_descent(batch_size, learning_rate, epoch):
    print_ln("here 1")


    print_ln("batch size: %s ", batch_size)


    partial_d = sfix.Matrix(max_dim_largest_layer, max_dim_largest_layer)
    partial_d2 = sfloat.Array(max_dim_largest_layer)

    @for_range(tot_num_hidden_layers)
    def _(layer):
        # print_ln("here 2")
        # print_ln("layer dim[0]: %s ", layer_dimensions[layer][0])
        @for_range(layer_dimensions[layer][0])
        def _(i):
            # print_ln("layer dim[1]: %s ", layer_dimensions[layer][1])
            # print_ln("here6")
            @for_range(layer_dimensions[layer][1])
            def _(j):
                # print_ln("here 3")
                # print_ln("j: %s", j)
                # print_ln("a[%s][%s]: %s", i, j, sfloat(adjustments[layer][i][j]).reveal())
                value = sfloat(0)
                value = sfloat(adjustments[layer][i][j])

                # if_then(j == 0)
                # print_ln("adjustments[%s][%s]: %s", i, j, sfloat(adjustments[layer][i][j]).reveal())
                # print_ln("batch size in: %s", batch_size)
                # print_ln("1/batch: %s", sfloat(1/batch_size).reveal())
                # print_ln("1/batch sfloat: %s", (sfloat(1)/sfloat(batch_size)).reveal())
                # end_if()

                # float matrix broken. Hence, convert back to sfix until fix is published
                # partial_d[i][j] = sfix((sfloat(1/batch_size)) * sfloat(adjustments[layer][i][j]))
                partial_d[i][j] = sfix((sfloat(1)/sfloat(batch_size)) * sfloat(adjustments[layer][i][j]))

                if_then(j == 0)
                print_ln("partial d[%s][%s]: %s", i, j, partial_d[i][j].reveal())
                end_if()
                if_then(j == 0)
                print_ln("part + learn rate[%s][%s]: %s", i, j, (sfloat(learning_rate) * -sfloat(partial_d[i][j])).reveal())
                end_if()
                weights[layer][i][j] = sfix(sfloat(weights[layer][i][j]) + (sfloat(learning_rate) * -sfloat(partial_d[i][j])))
                if_then(j == 0)
                print_ln("weights layer value[%s][%s][%s]: %s", layer, i, j, weights[layer][i][j].reveal())
                end_if()

                # print_ln("here 4")

                partial_d2[j] = (sfloat(1)/sfloat(batch_size)) * sfloat(adjustments[layer][layer_dimensions[layer][0]-1][j])
                biases[layer][j] = sfix(sfloat(biases[layer][j]) + (sfloat(learning_rate*0.001) * -partial_d2[j]))
                # print_ln("here 5")

        # if_then(epoch == 61)
        print_ln("+++++++++++weights++++++++++++++")
        @for_range(2)
        def _(i):
            @for_range(9)
            def _(j):
                @for_range(9)
                def _(k):
                    print_ln("weights[%s][%s][%s]: %s", i, j, k, weights[i][j][k].reveal())

        print_ln("+++++++++++biases++++++++++++++")
        @for_range(2)
        def _(j):
            @for_range(9)
            def _(k):
                print_ln("biases[%s][%s]: %s", j, k, biases[j][k].reveal())
        # end_if()

########################
# Number of layers and dimensions of largest layer have to be predefined so
# arithmetic circuit can be created accortingly
tot_num_layers = 3
# Output layer included in hidden layers
tot_num_hidden_layers = tot_num_layers - 1

# largest dimension of largest layer`
max_dim_largest_layer = 9

# TODO: make concise, when passing matrix in function and when just using
# it on a global scope
# Initialization of weights and biases
weights = sfix.MultiArray([tot_num_hidden_layers, max_dim_largest_layer, max_dim_largest_layer])
biases = sfix.Matrix(tot_num_hidden_layers, max_dim_largest_layer)
# num layers 3, data dim 2, layer dim 9
# adjustments = sfix.MultiArray([tot_num_hidden_layers, in_largest_layer, neurons_largest_layer])

# Dimensions of layers [n][0]: layer n in, [n][1]: layer n neurons
layer_dimensions = cint.Matrix(tot_num_hidden_layers, 2)
layer_dimensions[0][0] = 2
layer_dimensions[0][1] = 9
layer_dimensions[1][0] = 9
layer_dimensions[1][1] = 1

# Initiate count of added hidden layer
num_hidden_layer = MemValue(0)
add_layer(2,9)
print_ln("layer no: %s", num_hidden_layer)
add_layer(9,1)
print_ln("layer no: %s", num_hidden_layer)

###############################

# Set length and width (options) of training set
length_training_set = 4
size_input_training_set = 2
size_output_training_set = 1

# Create training data
training_data = sfix.Matrix(length_training_set,size_input_training_set)
training_labels = sfix.Matrix(length_training_set, size_output_training_set)

# 0,0 -> 0
training_data[0][0] = 0
training_data[0][1] = 0
training_labels[0][0] = 0
# 0,1 -> 1
training_data[1][0] = 0
training_data[1][1] = 1
training_labels[1][0] = 1
# 1,0 -> 1
training_data[2][0] = 1
training_data[2][1] = 0
training_labels[2][0] = 1
# 1,1 -> 0
training_data[3][0] = 1
training_data[3][1] = 1
training_labels[3][0] = 0

# input_iteration = 3
# @for_range(9)
# def _(i):
#     print_ln("input iteration: %s", input_iteration)
#     print_ln("z[%s]: %s", i, training_data[input_iteration][i].reveal())
# activ_val are basically the neurons, so contrary to the 3D list in python,
# a matrix is enough.
activ_val = sfix.Matrix(tot_num_layers, max_dim_largest_layer)
adjustments = sfix.MultiArray([tot_num_hidden_layers, max_dim_largest_layer, max_dim_largest_layer])

# train()
# Initiate error array
# TODO: make CustVector out of this
error = sfix.Array(500)
length_error = MemValue(0)
# TODO: in original code, this iterates from 0 to 3. That makes batch_size in
# gradient_decent 3, not 4. -> check if thisis correct or bug in original code
i_counter = MemValue(0)

# no of epochs
@for_range(2)
def _(k):
    i_counter.write(0)
    print_ln("+++++++++++++++++epoch no (%s)+++++++++++++++++++++++++", k)
    @for_range(length_training_set)
    def _(i):
        print_ln("------------next dataset (%s)------------------", i)

        # Forward pass of training set
        __forward_propagate(training_data, i, activ_val)

        print_ln("++++++++++++++++++++++++++++++++")
        @for_range(tot_num_layers)
        def _(j):
            print_ln("act_val[%s]: %s, %s, %s, %s, %s, %s", j, activ_val[j][0].reveal(), activ_val[j][1].reveal(), activ_val[j][2].reveal(), activ_val[j][3].reveal(), activ_val[j][4].reveal(), activ_val[j][8].reveal())

        print_ln("layer dim xxx: %s", layer_dimensions[tot_num_hidden_layers][1])

        out = sfix.Array(size_output_training_set)
        targ = sfix.Array(size_output_training_set)

        @for_range(size_output_training_set)
        def _(m):
            out[m] = activ_val[tot_num_layers-1][m]
            targ[m] = training_labels[i][m]

        print_ln("act_val tt %s", activ_val[2][0].reveal())
        print_ln("outtt %s", out[0].reveal())
        loss = sum_squared_error(out, targ)
        print_ln("loss %s", loss.reveal())
        # must append here, so not only i but i+k
        error[length_error] = loss
        length_error.write(length_error + 1)

        # Back propagation
        __back_propagate(activ_val, targ)
        print_ln("++++++++++++++++++++++++++++++++")
        i_counter.write(i_counter + 1)

    print_ln("length_error: %s", length_error)
    learning_rate = 1

    # count = cint(0)
    # count = i_counter
    #
    # print_ln("i_counter: %s", i_counter)
    # print_ln("count: %s", count)
    # i_counter-1 for now as implemented in original code. To check
    __gradient_descent(length_training_set-1, learning_rate, k)

    @for_range(26)
    def _(i):
        print_ln("Error_list[%s] %s", i, error[i].reveal())




#################################
# f = sfix.Array(5)
#
# def test(check, d):
#     print_ln("yup")
#     # d = sfix.Array(5)
#     d[0] = 1
#     d[1] = 2
#     d[2] = 3
#     d[3] = 4
#     d[4] = 5
#     # return d
#
# @for_range(2)
# def _(i):
#     test(i, f)
#
# @for_range(5)
# def _(i):
#     print_ln("f[%s]: %s", i, f[i].reveal())
#


###############################
# n = 10
# x = sfix.Matrix(2,n)
#
# @for_range(n)
# def _(i):
#     x[0][i] = sfix(sfix(i)+ sfix(1.1))
#
# def test(matrix):
#     matrix[0][3] = 1234
#     c = sfix.Matrix(2,n)
#     c[0][0] = 1432
#     return c
#
# y = test(x)
# @for_range(n)
# def _(i):
#     print_ln("%s", y[0][i].reveal())
###################################


# print_ln("w_1[%s] %s ", 1, weights[0][0][1].reveal())
# print_ln("w_1[%s] %s ", 2, weights[1][0][0].reveal())
# print_ln("++++++++++++++++")
# @for_range(9)
# def _(i):
#     print_ln("w_1[%s] %s ", i, weights[0][i][0].reveal())
# @for_range(9)
# def _(i):
#     print_ln("w_2[%s]: %s", i, weights[1][i][0].reveal())
#
# @for_range(9)
# def _(i):
#     print_ln("b[%s]: %s", i, biases[1][i].reveal())
# print_ln("++++++++++++++++")
########################

# def create_rand_sfloat

# print_ln("len %s", c[1][1].reveal())
#
# a[1][1] = sint(12)
# c[1][1] = sfloat(12)

# @for_range(len(a[0]))
# def _(i):
#     a[0][i] = sfloat(12.1)
#     a[1][i] = sfloat(13.1)

# print_ln("+++++++")
#
# @for_range(len(a[0]))
# def _(i):
#     print_ln("%s", a[0][i].reveal())
#
# print_ln("----------------")
# @for_range(len(a[0]))
# def _(i):
#     print_ln("%s", a[1][i].reveal())
#     print_ln("this is b %s", b.reveal())

print_ln("+++++++")


# n = 3
# m = 4
# p = 5
# a = sfloat.MultiArray([n,m,p])
# b = sfloat.MultiArray([n,m,p])
# c = sfloat.MultiArray([n,m,p])
# # for i in range(n):
# @for_range(n)
# def _(i):
#     @for_range(m)
#     def _(j):
#         @for_range(p)
#         def _(k):
# # for j in range(m):
#     # for k in range(p):
#             a[i][j][k] = 12.4
#             b[i][j][k] = 2 * (i + j + k)
#             c[i][j][k] = (a[i][j][k] + b[i][j][k])
#
# @for_range(3)
# def _(i):
#     print_ln("%s", a[1][1][i].reveal())

###########################
# Test add_layer()
# weights, b, adjustments = add_layer()
#
# print_ln("===================")
# @for_range(9)
# def _(i):
#     print_ln("w[%s] %s ", i, weights[0][i].reveal())
# print_ln("")
# @for_range(9)
# def _(i):
#     print_ln("w[%s]: %s", i, weights[1][i].reveal())
#
# print(b)
# @for_range(9)
# def _(i):
#     print_ln("b[%s]: %s", i, b[0][i].reveal())
#
# @for_range(9)
# def _(k):
#     print_ln("a[%s]: %s", k, adjustments[0][0][k].reveal())
#
# print_ln("===================")

###############################


# ###############################
# # FOR REPORT : comparison everything converting in sfloat or stay in sfix
#
# start_timer(1)
# y = sfix(4)
# sigmoid = sfix(1) / (sfix(1) + mpc_math.exp2_fx(y * mpc_math.log2_fx(sfix(EULER))))
# print_ln("sigmoid %s", sigmoid.reveal())
# stop_timer(1)
#
# start_timer(2)
# y2 = sfloat(4)
# sigmoid2 = sfloat(1) / (sfloat(1) + sfloat(mpc_math.exp2_fx(sfix(y * sfloat(mpc_math.log2_fx(sfix(EULER)))))))
# print_ln("sigmoid2 %s", sigmoid2.reveal())
# stop_timer(2)
#
#
# ###############################



#
